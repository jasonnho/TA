{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0_imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.5.1+cu121\n",
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "VRAM: 8.0 GB\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import string\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from torchcrf import CRF\n",
    "from seqeval.metrics import classification_report, f1_score as seq_f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m1_config",
   "metadata": {},
   "source": [
    "### 1. KONFIGURASI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1_config",
   "metadata": {},
   "outputs": [],
   "source": "# Paths\nBASE_DIR = os.path.dirname(os.getcwd())\nDATA_RAW_DIR = os.path.join(BASE_DIR, 'data', 'raw')\nDATA_PROC_DIR = os.path.join(BASE_DIR, 'data', 'processed')\nMODEL_DIR = os.path.join(BASE_DIR, 'models')\nos.makedirs(MODEL_DIR, exist_ok=True)\n\nBIEOS_PATH = os.path.join(DATA_PROC_DIR, 'train_data_bieos.json')\nPOS_LEX_PATH = os.path.join(DATA_RAW_DIR, 'indonesian_sentiment_lexicon_positive.tsv')\nNEG_LEX_PATH = os.path.join(DATA_RAW_DIR, 'indonesian_sentiment_lexicon_negative.tsv')\n\n# Model\nMODEL_NAME = 'indobenchmark/indobert-large-p2'\nMAX_LENGTH = 128\nPROJ_DIM = 256\n\n# Training\nBATCH_SIZE = 4\nGRADIENT_ACCUMULATION = 4  # effective batch = 16\nNUM_EPOCHS = 15\nLR_BERT = 2e-5\nLR_HEAD = 1e-4\nWARMUP_RATIO = 0.1\nWEIGHT_DECAY = 0.01\nMAX_GRAD_NORM = 1.0\nTRAIN_RATIO = 0.85\nSEED = 42\n\n# Architecture (Paper-aligned)\nDROPOUT = 0.1\nPHASE1_EPOCHS = 3           # SLD pre-training for Sentiment Connection\nLAMBDA1 = 0.3               # Paper Eq. 10: weight for L_ae + L_sl\nLAMBDA2 = 0.3               # Paper Eq. 10: weight for L_sd\nMAX_REL_POS = 20            # Max relative position for cross-attention\n\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nprint(f'Model       : {MODEL_NAME}')\nprint(f'Batch size  : {BATCH_SIZE} x {GRADIENT_ACCUMULATION} accum = {BATCH_SIZE * GRADIENT_ACCUMULATION} effective')\nprint(f'Training    : Phase 1 ({PHASE1_EPOCHS} epochs) + Phase 2 ({NUM_EPOCHS} epochs)')\nprint(f'LR (BERT)   : {LR_BERT}')\nprint(f'LR (heads)  : {LR_HEAD}')\nprint(f'Loss weight : λ1={LAMBDA1}, λ2={LAMBDA2} (paper Eq. 10)')"
  },
  {
   "cell_type": "markdown",
   "id": "m2_load",
   "metadata": {},
   "source": [
    "### 2. LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2_load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 2451\n",
      "Positive lexicon : 2288 words\n",
      "Negative lexicon : 5025 words\n",
      "Overlap removed  : 1081 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded : indobenchmark/indobert-large-p2\n"
     ]
    }
   ],
   "source": [
    "# Load raw BIEOS data (original tokens + labels)\n",
    "with open(BIEOS_PATH, 'r', encoding='utf-8') as f:\n",
    "    raw_data = json.load(f)\n",
    "print(f'Total data: {len(raw_data)}')\n",
    "\n",
    "# Load sentiment lexicon\n",
    "def load_lexicon(path):\n",
    "    words = set()\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        next(f)  # skip header\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if parts:\n",
    "                word = parts[0].strip().lower()\n",
    "                if ' ' not in word and len(word) > 1:  # single-word entries only\n",
    "                    words.add(word)\n",
    "    return words\n",
    "\n",
    "pos_lexicon = load_lexicon(POS_LEX_PATH)\n",
    "neg_lexicon = load_lexicon(NEG_LEX_PATH)\n",
    "\n",
    "# Hapus kata yang muncul di kedua lexicon (ambigu)\n",
    "overlap = pos_lexicon & neg_lexicon\n",
    "pos_lexicon -= overlap\n",
    "neg_lexicon -= overlap\n",
    "\n",
    "print(f'Positive lexicon : {len(pos_lexicon)} words')\n",
    "print(f'Negative lexicon : {len(neg_lexicon)} words')\n",
    "print(f'Overlap removed  : {len(overlap)} words')\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f'Tokenizer loaded : {MODEL_NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m3_labels",
   "metadata": {},
   "source": [
    "### 3. MULTI-TASK LABEL PREPARATION\n",
    "\n",
    "| Task | Label Scheme | Jumlah |\n",
    "|---|---|---|\n",
    "| **ATE** (Aspect Term Extraction) | O, B, I, E, S | 5 |\n",
    "| **SLD** (Sentiment Lexicon Detection) | O, POS, NEG | 3 |\n",
    "| **ASD** (Aspect Sentiment Detection) | O, POS, NEG, NEU | 4 |\n",
    "| **Final** (Aspect Polarity - CRF) | O, B/I/E/S-POS/NEG/NEU | 13 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3_labels",
   "metadata": {},
   "outputs": [],
   "source": "# Label schemes\nate_labels = ['O', 'B', 'I', 'E', 'S']\nsld_labels = ['O', 'POS', 'NEG']\nasd_labels = ['O', 'POS', 'NEG', 'NEU']\nbieos_labels = ['O', 'B-NEG', 'B-NEU', 'B-POS', 'E-NEG', 'E-NEU', 'E-POS',\n                'I-NEG', 'I-NEU', 'I-POS', 'S-NEG', 'S-NEU', 'S-POS']\n\nate_label2id = {l: i for i, l in enumerate(ate_labels)}\nsld_label2id = {l: i for i, l in enumerate(sld_labels)}\nasd_label2id = {l: i for i, l in enumerate(asd_labels)}\nbieos_label2id = {l: i for i, l in enumerate(bieos_labels)}\nbieos_id2label = {i: l for l, i in bieos_label2id.items()}\n\nIGNORE_INDEX = -100\n\n# --- Prior Embedding: word-level ATE frequency distribution (Paper Sec. 3.1.4) ---\nword_ate_freq = {}\nfor d in raw_data:\n    for tok, lab in zip(d['tokens'], d['labels']):\n        tok_lower = tok.lower()\n        if tok_lower not in word_ate_freq:\n            word_ate_freq[tok_lower] = [0] * len(ate_labels)\n        if lab == 'O':\n            word_ate_freq[tok_lower][ate_label2id['O']] += 1\n        else:\n            prefix = lab.split('-')[0]\n            word_ate_freq[tok_lower][ate_label2id[prefix]] += 1\n\ndefault_prior = [1.0] + [0.0] * (len(ate_labels) - 1)\nword_prior = {}\nfor word, counts in word_ate_freq.items():\n    total = sum(counts)\n    word_prior[word] = [c / total for c in counts] if total > 0 else default_prior\n\nprint(f'Prior vocabulary: {len(word_prior)} unique words')\n\n# --- Prepare aligned labels for all 4 tasks + prior probs ---\nall_input_ids = []\nall_attention_mask = []\nall_ate = []\nall_sld = []\nall_asd = []\nall_bieos = []\nall_crf = []\nall_prior = []\n\nfor d in tqdm(raw_data, desc='Preparing labels'):\n    tokens, labels = d['tokens'], d['labels']\n\n    enc = tokenizer(tokens, is_split_into_words=True,\n                    max_length=MAX_LENGTH, padding='max_length',\n                    truncation=True, return_tensors='pt')\n    word_ids = enc.word_ids(batch_index=0)\n\n    ate_w, sld_w, asd_w = [], [], []\n    for tok, lab in zip(tokens, labels):\n        if lab == 'O':\n            ate_w.append('O'); asd_w.append('O')\n        else:\n            prefix, sentiment = lab.split('-', 1)\n            ate_w.append(prefix); asd_w.append(sentiment)\n\n        t_clean = tok.lower().strip(string.punctuation)\n        if t_clean in pos_lexicon:\n            sld_w.append('POS')\n        elif t_clean in neg_lexicon:\n            sld_w.append('NEG')\n        else:\n            sld_w.append('O')\n\n    ate_a, sld_a, asd_a, bieos_a, crf_a, prior_a = [], [], [], [], [], []\n    prev_wid = None\n    for wid in word_ids:\n        if wid is None:\n            ate_a.append(IGNORE_INDEX)\n            sld_a.append(IGNORE_INDEX)\n            asd_a.append(IGNORE_INDEX)\n            bieos_a.append(IGNORE_INDEX)\n            crf_a.append(0)\n            prior_a.append(default_prior)\n        elif wid != prev_wid:\n            if wid < len(tokens):\n                ate_a.append(ate_label2id[ate_w[wid]])\n                sld_a.append(sld_label2id[sld_w[wid]])\n                asd_a.append(asd_label2id[asd_w[wid]])\n                bieos_a.append(bieos_label2id[labels[wid]])\n                crf_a.append(bieos_label2id[labels[wid]])\n                prior_a.append(word_prior.get(tokens[wid].lower(), default_prior))\n            else:\n                ate_a.append(IGNORE_INDEX); sld_a.append(IGNORE_INDEX)\n                asd_a.append(IGNORE_INDEX); bieos_a.append(IGNORE_INDEX)\n                crf_a.append(0)\n                prior_a.append(default_prior)\n        else:\n            ate_a.append(IGNORE_INDEX); sld_a.append(IGNORE_INDEX)\n            asd_a.append(IGNORE_INDEX); bieos_a.append(IGNORE_INDEX)\n            crf_a.append(bieos_label2id[labels[wid]] if wid < len(tokens) else 0)\n            prior_a.append(word_prior.get(tokens[wid].lower(), default_prior) if wid < len(tokens) else default_prior)\n        prev_wid = wid\n\n    all_input_ids.append(enc['input_ids'].squeeze(0))\n    all_attention_mask.append(enc['attention_mask'].squeeze(0))\n    all_ate.append(torch.tensor(ate_a, dtype=torch.long))\n    all_sld.append(torch.tensor(sld_a, dtype=torch.long))\n    all_asd.append(torch.tensor(asd_a, dtype=torch.long))\n    all_bieos.append(torch.tensor(bieos_a, dtype=torch.long))\n    all_crf.append(torch.tensor(crf_a, dtype=torch.long))\n    all_prior.append(torch.tensor(prior_a, dtype=torch.float))\n\ndata_dict = {\n    'input_ids': torch.stack(all_input_ids),\n    'attention_mask': torch.stack(all_attention_mask),\n    'ate_labels': torch.stack(all_ate),\n    'sld_labels': torch.stack(all_sld),\n    'asd_labels': torch.stack(all_asd),\n    'bieos_labels': torch.stack(all_bieos),\n    'crf_labels': torch.stack(all_crf),\n    'prior_probs': torch.stack(all_prior),\n}\n\nprint('\\nDataset shapes:')\nfor k, v in data_dict.items():\n    print(f'  {k}: {v.shape}')\n\n# Cek SLD coverage\nsld_flat = data_dict['sld_labels'].flatten()\nsld_valid = sld_flat[sld_flat != IGNORE_INDEX]\nsld_counts = Counter(sld_valid.tolist())\nprint(f'\\nSLD coverage:')\nfor lid, count in sorted(sld_counts.items()):\n    print(f'  {sld_labels[lid]:4s}: {count}')"
  },
  {
   "cell_type": "markdown",
   "id": "m4_dataset",
   "metadata": {},
   "source": [
    "### 4. DATASET & DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4_dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2083 (85%)\n",
      "Val  : 368 (15%)\n",
      "Train batches: 521\n"
     ]
    }
   ],
   "source": [
    "class ABSAMultiTaskDataset(Dataset):\n",
    "    def __init__(self, data_dict, indices):\n",
    "        self.data = {k: v[indices] for k, v in data_dict.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data['input_ids'].size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "# Train/Val split\n",
    "total = len(raw_data)\n",
    "indices = torch.randperm(total, generator=torch.Generator().manual_seed(SEED))\n",
    "split = int(total * TRAIN_RATIO)\n",
    "\n",
    "train_dataset = ABSAMultiTaskDataset(data_dict, indices[:split])\n",
    "val_dataset = ABSAMultiTaskDataset(data_dict, indices[split:])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE * 2)\n",
    "\n",
    "print(f'Train: {len(train_dataset)} ({TRAIN_RATIO*100:.0f}%)')\n",
    "print(f'Val  : {len(val_dataset)} ({(1-TRAIN_RATIO)*100:.0f}%)')\n",
    "print(f'Train batches: {len(train_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m5_model",
   "metadata": {},
   "source": "### 5. MODEL ARCHITECTURE (Paper-aligned)\n\nSesuai Wang et al. (2021), adaptasi: IndoBERT menggantikan BiLSTM sebagai encoder.\n\n```\nIndoBERT (shared encoder, all layers trainable)\n        |\n        h\n       / \\\n  [h;prior] h\n      |      |\n    h_ae   h_sl\n     |       |\n  ATE cls  SLD cls         ← CE loss (λ1=0.3)\n     |       |\n  CrossAttn(h_ae → h_sl)   ← Paper Eq. 7-9 (with position encoding)\n        |\n       h_sd\n        |\n     ASD cls                ← CE loss (λ2=0.3)\n        |\n  [h_ae ; h_sd]\n        |\n      CRF                   ← NLL loss (λ=1.0)\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5_model",
   "metadata": {},
   "outputs": [],
   "source": "class CrossAttentionASD(nn.Module):\n    \"\"\"Paper Eq. 7-9: Cross-attention from ATE to SLD with relative position encoding.\"\"\"\n\n    def __init__(self, dim, max_relative_position=20):\n        super().__init__()\n        self.max_rel_pos = max_relative_position\n        self.pos_embedding = nn.Embedding(2 * max_relative_position + 1, dim)\n        self.W_ae = nn.Linear(dim, dim, bias=False)\n        self.W_sl = nn.Linear(dim, dim, bias=False)\n        self.W_pos = nn.Linear(dim, dim, bias=False)\n        self.bias = nn.Parameter(torch.zeros(dim))\n        self.w_s = nn.Linear(dim, 1, bias=False)\n\n    def forward(self, h_ae, h_sl, mask=None):\n        batch, seq_len, dim = h_ae.shape\n        pos = torch.arange(seq_len, device=h_ae.device)\n        rel_pos = (pos.unsqueeze(0) - pos.unsqueeze(1)).clamp(\n            -self.max_rel_pos, self.max_rel_pos) + self.max_rel_pos\n        p_ij = self.pos_embedding(rel_pos)\n\n        s_ae = self.W_ae(h_ae)\n        s_sl = self.W_sl(h_sl)\n        s_pos = self.W_pos(p_ij)\n\n        combined = s_ae.unsqueeze(2) + s_sl.unsqueeze(1) + s_pos.unsqueeze(0) + self.bias\n        scores = self.w_s(torch.tanh(combined)).squeeze(-1)\n\n        if mask is not None:\n            scores = scores.masked_fill(~mask.unsqueeze(1), float('-inf'))\n\n        attn_weights = torch.softmax(scores, dim=-1)\n        h_sd = torch.bmm(attn_weights, h_sl)\n        return h_sd\n\n\nclass HierarchicalMultiTaskABSA(nn.Module):\n    def __init__(self, model_name, num_ate, num_sld, num_asd, num_final,\n                 proj_dim=256, dropout=0.1, asd_weights=None,\n                 max_relative_position=20, lambda1=0.3, lambda2=0.3):\n        super().__init__()\n        self.lambda1 = lambda1\n        self.lambda2 = lambda2\n        self.num_sld = num_sld\n        self.num_asd = num_asd\n\n        self.bert = AutoModel.from_pretrained(model_name, use_safetensors=True, low_cpu_mem_usage=True)\n        bert_dim = self.bert.config.hidden_size\n\n        # Task 1: ATE with Prior Embedding\n        self.ate_proj = nn.Sequential(\n            nn.Linear(bert_dim + num_ate, proj_dim), nn.GELU(), nn.Dropout(dropout))\n        self.ate_classifier = nn.Linear(proj_dim, num_ate)\n\n        # Task 2: SLD\n        self.sld_proj = nn.Sequential(\n            nn.Linear(bert_dim, proj_dim), nn.GELU(), nn.Dropout(dropout))\n        self.sld_classifier = nn.Linear(proj_dim, num_sld)\n\n        # Task 3: ASD with Cross-Attention (Paper Eq. 7-9)\n        self.cross_attention = CrossAttentionASD(proj_dim, max_relative_position)\n        self.asd_classifier = nn.Linear(proj_dim, num_asd)\n\n        # Task 4: CRF\n        self.final_proj = nn.Sequential(\n            nn.Linear(proj_dim * 2, proj_dim), nn.GELU(), nn.Dropout(dropout))\n        self.final_emission = nn.Linear(proj_dim, num_final)\n        self.crf = CRF(num_final, batch_first=True)\n\n        self.dropout = nn.Dropout(dropout)\n        self.asd_weights = asd_weights\n\n    def forward(self, input_ids, attention_mask, prior_probs=None,\n                ate_labels=None, sld_labels=None, asd_labels=None, crf_labels=None, **kwargs):\n        h = self.dropout(self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state)\n\n        # Task 1: ATE with Prior Embedding\n        if prior_probs is not None:\n            h_with_prior = torch.cat([h, prior_probs], dim=-1)\n        else:\n            h_with_prior = torch.cat([h, torch.zeros(*h.shape[:-1], self.ate_classifier.out_features, device=h.device)], dim=-1)\n        h_ae = self.ate_proj(h_with_prior)\n        ate_logits = self.ate_classifier(h_ae)\n\n        # Task 2: SLD\n        h_sl = self.sld_proj(h)\n        sld_logits = self.sld_classifier(h_sl)\n\n        # Task 3: ASD — Cross-Attention (h_ae → h_sl)\n        mask = attention_mask.bool()\n        h_sd = self.cross_attention(h_ae, h_sl, mask=mask)\n        asd_logits = self.asd_classifier(h_sd)\n\n        # Task 4: CRF\n        h_concat = torch.cat([h_ae, h_sd], dim=-1)\n        emissions = self.final_emission(self.final_proj(h_concat))\n\n        outputs = {'ate_logits': ate_logits, 'sld_logits': sld_logits,\n                   'asd_logits': asd_logits, 'emissions': emissions}\n\n        if ate_labels is not None:\n            ce = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n            loss_ate = ce(ate_logits.view(-1, ate_logits.size(-1)), ate_labels.view(-1))\n            loss_sld = ce(sld_logits.view(-1, sld_logits.size(-1)), sld_labels.view(-1))\n\n            ce_asd = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX,\n                                          weight=self.asd_weights) if self.asd_weights is not None else ce\n            loss_asd = ce_asd(asd_logits.view(-1, asd_logits.size(-1)), asd_labels.view(-1))\n\n            loss_crf = -self.crf(emissions.float(), crf_labels, mask=mask, reduction='mean')\n\n            # Paper Eq. 10: L = λ1*(L_ae + L_sl) + λ2*L_sd + L_co\n            outputs['loss'] = self.lambda1 * (loss_ate + loss_sld) + self.lambda2 * loss_asd + loss_crf\n            outputs['losses'] = {\n                'ate': loss_ate.detach(), 'sld': loss_sld.detach(),\n                'asd': loss_asd.detach(), 'crf': loss_crf.detach()}\n\n        return outputs\n\n    def decode(self, emissions, attention_mask):\n        return self.crf.decode(emissions.float(), mask=attention_mask.bool())\n\n    def apply_sentiment_connection(self):\n        \"\"\"Copy SLD classifier weights to ASD classifier (Paper Sec. 3.1.5).\"\"\"\n        with torch.no_grad():\n            n_copy = min(self.num_sld, self.num_asd)\n            self.asd_classifier.weight.data[:n_copy].copy_(self.sld_classifier.weight.data[:n_copy])\n            self.asd_classifier.bias.data[:n_copy].copy_(self.sld_classifier.bias.data[:n_copy])\n        print(f'Sentiment Connection: copied SLD -> ASD weights (classes 0-{n_copy-1}: O, POS, NEG)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b_init",
   "metadata": {},
   "outputs": [],
   "source": "model = HierarchicalMultiTaskABSA(\n    model_name=MODEL_NAME,\n    num_ate=len(ate_labels),\n    num_sld=len(sld_labels),\n    num_asd=len(asd_labels),\n    num_final=len(bieos_labels),\n    proj_dim=PROJ_DIM,\n    dropout=DROPOUT,\n    max_relative_position=MAX_REL_POS,\n    lambda1=LAMBDA1,\n    lambda2=LAMBDA2,\n).to(device)\n\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'Total parameters    : {total_params:,}')\nprint(f'Trainable parameters: {trainable:,}')\nprint(f'BERT hidden size    : {model.bert.config.hidden_size}')\nprint(f'Cross-Attention ASD : {sum(p.numel() for p in model.cross_attention.parameters()):,} params')"
  },
  {
   "cell_type": "markdown",
   "id": "m6_train",
   "metadata": {},
   "source": "### 6. TRAINING (Two-Phase + Sentiment Connection)\n\n- **Phase 1** (3 epochs): Pre-training — semua task aktif, SLD classifier dilatih\n- **Sentiment Connection**: Copy SLD classifier weights → ASD classifier\n- **Phase 2** (15 epochs): Main training — baseline (no early stopping, no freeze, no class weights)"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6_eval_fn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device, desc='Evaluating'):\n",
    "    \"\"\"Evaluate model: compute loss + entity-level F1 from CRF predictions.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_true = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=desc, leave=False):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            total_loss += outputs['loss'].item()\n",
    "\n",
    "            # CRF decode\n",
    "            preds = model.decode(outputs['emissions'], batch['attention_mask'])\n",
    "            bieos_lbl = batch['bieos_labels']\n",
    "\n",
    "            # Ambil hanya posisi first-subword (bieos_labels != -100)\n",
    "            for i in range(len(preds)):\n",
    "                pred_seq, true_seq = [], []\n",
    "                for j in range(len(preds[i])):\n",
    "                    if bieos_lbl[i][j].item() != IGNORE_INDEX:\n",
    "                        pred_seq.append(bieos_id2label[preds[i][j]])\n",
    "                        true_seq.append(bieos_id2label[bieos_lbl[i][j].item()])\n",
    "                all_preds.append(pred_seq)\n",
    "                all_true.append(true_seq)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    f1 = seq_f1_score(all_true, all_preds)\n",
    "    return avg_loss, f1, all_true, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7_train",
   "metadata": {},
   "outputs": [],
   "source": "# Optimizer: different LR for BERT vs task heads\nbert_params = list(model.bert.named_parameters())\nhead_params = [(n, p) for n, p in model.named_parameters() if not n.startswith('bert')]\n\ntotal_training_epochs = PHASE1_EPOCHS + NUM_EPOCHS\ntotal_steps = (len(train_loader) // GRADIENT_ACCUMULATION) * total_training_epochs\nwarmup_steps = int(total_steps * WARMUP_RATIO)\n\noptimizer = torch.optim.AdamW([\n    {'params': [p for _, p in bert_params], 'lr': LR_BERT},\n    {'params': [p for _, p in head_params], 'lr': LR_HEAD},\n], weight_decay=WEIGHT_DECAY)\n\nscheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n\nprint(f'Total steps (P1+P2) : {total_steps}')\nprint(f'Warmup steps        : {warmup_steps}')\n\n# Training history\nhistory = {'train_loss': [], 'val_loss': [], 'val_f1': [],\n           'loss_ate': [], 'loss_sld': [], 'loss_asd': [], 'loss_crf': []}\n\ndef train_one_epoch(model, train_loader, optimizer, scheduler, device, epoch_str):\n    model.train()\n    epoch_loss = 0\n    epoch_comp = {'ate': 0, 'sld': 0, 'asd': 0, 'crf': 0}\n    optimizer.zero_grad()\n    t0 = time.time()\n\n    step_bar = tqdm(train_loader, desc=epoch_str, leave=False, unit='batch')\n    for step, batch in enumerate(step_bar):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs['loss'] / GRADIENT_ACCUMULATION\n        loss.backward()\n\n        if (step + 1) % GRADIENT_ACCUMULATION == 0 or (step + 1) == len(train_loader):\n            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n\n        epoch_loss += outputs['loss'].item()\n        for k in epoch_comp:\n            epoch_comp[k] += outputs['losses'][k].item()\n        step_bar.set_postfix(loss=f\"{epoch_loss/(step+1):.3f}\")\n\n    n = len(train_loader)\n    return epoch_loss / n, {k: v / n for k, v in epoch_comp.items()}, time.time() - t0\n\n# =============================================\n# PHASE 1: Pre-training (for Sentiment Connection)\n# =============================================\nprint('=' * 60)\nprint(f'PHASE 1: Pre-training ({PHASE1_EPOCHS} epochs)')\nprint('=' * 60)\n\nfor epoch in range(PHASE1_EPOCHS):\n    avg_train, comp, elapsed = train_one_epoch(\n        model, train_loader, optimizer, scheduler, device,\n        f'P1 Epoch {epoch+1}/{PHASE1_EPOCHS}')\n\n    val_loss, val_f1, _, _ = evaluate(model, val_loader, device, desc='Validating')\n\n    history['train_loss'].append(avg_train)\n    history['val_loss'].append(val_loss)\n    history['val_f1'].append(val_f1)\n    for k in comp:\n        history[f'loss_{k}'].append(comp[k])\n\n    tqdm.write(\n        f'P1 Epoch {epoch+1}/{PHASE1_EPOCHS} | {elapsed:.0f}s | '\n        f'Train: {avg_train:.4f} | Val: {val_loss:.4f} | F1: {val_f1:.4f}\\n'\n        f'  ATE:{comp[\"ate\"]:.3f}  SLD:{comp[\"sld\"]:.3f}  '\n        f'ASD:{comp[\"asd\"]:.3f}  CRF:{comp[\"crf\"]:.3f}')\n\n# =============================================\n# SENTIMENT CONNECTION: Copy SLD → ASD\n# =============================================\nprint('\\n' + '=' * 60)\nprint('Applying Sentiment Connection')\nprint('=' * 60)\nmodel.apply_sentiment_connection()\n\n# =============================================\n# PHASE 2: Main Training (Baseline — no early stopping)\n# =============================================\nprint('\\n' + '=' * 60)\nprint(f'PHASE 2: Main Training ({NUM_EPOCHS} epochs)')\nprint('=' * 60)\n\nbest_f1 = 0\n\nfor epoch in range(NUM_EPOCHS):\n    global_epoch = PHASE1_EPOCHS + epoch + 1\n\n    avg_train, comp, elapsed = train_one_epoch(\n        model, train_loader, optimizer, scheduler, device,\n        f'Epoch {global_epoch}/{PHASE1_EPOCHS + NUM_EPOCHS}')\n\n    val_loss, val_f1, _, _ = evaluate(model, val_loader, device, desc='Validating')\n\n    history['train_loss'].append(avg_train)\n    history['val_loss'].append(val_loss)\n    history['val_f1'].append(val_f1)\n    for k in comp:\n        history[f'loss_{k}'].append(comp[k])\n\n    improved = val_f1 > best_f1\n    if improved:\n        best_f1 = val_f1\n        torch.save(model.state_dict(), os.path.join(MODEL_DIR, 'best_model.pt'))\n\n    tqdm.write(\n        f'Epoch {global_epoch:2d}/{PHASE1_EPOCHS + NUM_EPOCHS} | {elapsed:.0f}s | '\n        f'Train: {avg_train:.4f} | Val: {val_loss:.4f} | '\n        f'F1: {val_f1:.4f} {\"*\" if improved else \"\"}\\n'\n        f'  ATE:{comp[\"ate\"]:.3f}  SLD:{comp[\"sld\"]:.3f}  '\n        f'ASD:{comp[\"asd\"]:.3f}  CRF:{comp[\"crf\"]:.3f}')\n\nactual_epochs = len(history['train_loss'])\nprint(f'\\nTraining finished after {actual_epochs} epochs')\nprint(f'  Phase 1: {PHASE1_EPOCHS} epochs (SLD pre-training)')\nprint(f'  Phase 2: {NUM_EPOCHS} epochs (main training)')\nprint(f'Best Val F1: {best_f1:.4f}')"
  },
  {
   "cell_type": "markdown",
   "id": "m7_viz",
   "metadata": {},
   "source": [
    "### 7. VISUALISASI TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8_viz",
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 3, figsize=(16, 4.5))\n\nepochs_range = range(1, len(history['train_loss']) + 1)\n\n# Loss curves\naxes[0].plot(epochs_range, history['train_loss'], label='Train', linewidth=2)\naxes[0].plot(epochs_range, history['val_loss'], label='Val', linewidth=2)\naxes[0].axvline(PHASE1_EPOCHS + 0.5, color='gray', linestyle=':', alpha=0.7, label='Sent. Connection')\naxes[0].set_title('Total Loss', fontweight='bold')\naxes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss')\naxes[0].legend()\n\n# F1 curve\naxes[1].plot(epochs_range, history['val_f1'], color='#2ecc71', linewidth=2, marker='o', markersize=4)\nbest_epoch = history['val_f1'].index(max(history['val_f1'])) + 1\naxes[1].axvline(best_epoch, color='#e74c3c', linestyle='--', alpha=0.5,\n                label=f'Best: {max(history[\"val_f1\"]):.4f} (epoch {best_epoch})')\naxes[1].axvline(PHASE1_EPOCHS + 0.5, color='gray', linestyle=':', alpha=0.7, label='Sent. Connection')\naxes[1].set_title('Validation F1 (Entity-level)', fontweight='bold')\naxes[1].set_xlabel('Epoch'); axes[1].set_ylabel('F1')\naxes[1].legend()\n\n# Per-task loss\nfor task in ['ate', 'sld', 'asd', 'crf']:\n    axes[2].plot(epochs_range, history[f'loss_{task}'], label=task.upper(), linewidth=1.5)\naxes[2].axvline(PHASE1_EPOCHS + 0.5, color='gray', linestyle=':', alpha=0.7, label='Sent. Connection')\naxes[2].set_title('Per-Task Loss (Train)', fontweight='bold')\naxes[2].set_xlabel('Epoch'); axes[2].set_ylabel('Loss')\naxes[2].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "m8_eval",
   "metadata": {},
   "source": [
    "### 8. EVALUASI FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9_eval",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446fd6701aff42e49aad3195fefd43a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model - Val Loss: 15.8271, Val F1: 0.7263\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.56      0.60      0.58        83\n",
      "         NEU       0.52      0.63      0.57       125\n",
      "         POS       0.75      0.83      0.79       536\n",
      "\n",
      "   micro avg       0.69      0.77      0.73       744\n",
      "   macro avg       0.61      0.69      0.65       744\n",
      "weighted avg       0.69      0.77      0.73       744\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(os.path.join(MODEL_DIR, 'best_model.pt'), weights_only=True))\n",
    "val_loss, val_f1, all_true, all_preds = evaluate(model, val_loader, device)\n",
    "\n",
    "print(f'Best Model - Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}')\n",
    "print(f'\\n{classification_report(all_true, all_preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m9_save",
   "metadata": {},
   "source": [
    "### 9. SIMPAN MODEL & CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10_save",
   "metadata": {},
   "outputs": [],
   "source": "# Save complete checkpoint\ncheckpoint = {\n    'model_state_dict': model.state_dict(),\n    'model_name': MODEL_NAME,\n    'max_length': MAX_LENGTH,\n    'proj_dim': PROJ_DIM,\n    'dropout': DROPOUT,\n    'bieos_label2id': bieos_label2id,\n    'bieos_id2label': bieos_id2label,\n    'ate_labels': ate_labels,\n    'sld_labels': sld_labels,\n    'asd_labels': asd_labels,\n    'best_f1': best_f1,\n    'history': history,\n    'config': {\n        'architecture': 'Paper-aligned (cross-attention, prior embedding, sentiment connection)',\n        'dropout': DROPOUT,\n        'weight_decay': WEIGHT_DECAY,\n        'freeze_layers': '0/24 (all trainable)',\n        'early_stopping': 'none',\n        'asd_class_weights': 'none',\n        'lambda1': LAMBDA1,\n        'lambda2': LAMBDA2,\n        'phase1_epochs': PHASE1_EPOCHS,\n    }\n}\n\nsave_path = os.path.join(MODEL_DIR, 'checkpoint_final.pt')\ntorch.save(checkpoint, save_path)\n\nfile_size = os.path.getsize(save_path) / (1024**2)\nprint(f'Checkpoint saved: {save_path}')\nprint(f'File size: {file_size:.1f} MB')\nprint(f'Best F1: {best_f1:.4f}')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TA NLP - RTX 4060)",
   "language": "python",
   "name": "ta_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}